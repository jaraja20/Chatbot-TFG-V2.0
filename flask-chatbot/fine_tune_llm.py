"""
Fine-tuning de llama-3.2-1b para clasificaci√≥n de intents
Usando unsloth para mayor velocidad y eficiencia
"""

import torch
from datasets import load_dataset
import os

print("üöÄ Iniciando Fine-tuning de llama-3.2-1b")
print("=" * 60)

# =====================================================
# PASO 1: Verificar GPU/CPU
# =====================================================
if torch.cuda.is_available():
    print(f"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}")
    device = "cuda"
else:
    print("‚ö†Ô∏è GPU no disponible, usando CPU (ser√° m√°s lento)")
    device = "cpu"

# =====================================================
# PASO 2: Cargar modelo base
# =====================================================
print("\nüì• Cargando modelo base...")

# Usamos TinyLlama: open-source, ~1B par√°metros, no requiere autenticaci√≥n
# Alternativas: microsoft/phi-2 (2.7B), distilgpt2 (small)
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

try:
    from unsloth import FastLanguageModel
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = MODEL_NAME,
        max_seq_length = 512,
        dtype = None,  # Auto-detecta
        load_in_4bit = True,  # Usa quantizaci√≥n 4-bit para ahorrar memoria
    )
    print(f"‚úÖ Modelo {MODEL_NAME} cargado con unsloth (optimizado)")
    
except ImportError:
    print("‚ö†Ô∏è unsloth no disponible, usando transformers est√°ndar")
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    print(f"‚úÖ Modelo {MODEL_NAME} cargado con transformers")

# =====================================================
# PASO 3: Preparar modelo para fine-tuning (LoRA)
# =====================================================
print("\nüîß Configurando LoRA para fine-tuning eficiente...")

try:
    from peft import LoraConfig, get_peft_model, TaskType
    
    # Configuraci√≥n LoRA (Low-Rank Adaptation)
    lora_config = LoraConfig(
        r=16,  # Rango de la matriz LoRA
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    model = get_peft_model(model, lora_config)
    print("‚úÖ LoRA configurado (solo entrena ~1% de par√°metros)")
    
except ImportError:
    print("‚ö†Ô∏è PEFT no disponible, fine-tuning completo (m√°s lento)")

# =====================================================
# PASO 4: Cargar dataset
# =====================================================
print("\nüìä Cargando dataset de training...")

train_dataset = load_dataset('json', data_files='dataset_training_filtered.jsonl', split='train')
val_dataset = load_dataset('json', data_files='dataset_validation.jsonl', split='train')

print(f"‚úÖ Training: {len(train_dataset)} ejemplos")
print(f"‚úÖ Validation: {len(val_dataset)} ejemplos")

# =====================================================
# PASO 5: Configurar training
# =====================================================
print("\n‚öôÔ∏è Configurando par√°metros de training...")

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./llama-3.2-1b-intent-classifier",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=10,
    learning_rate=2e-4,
    fp16=True if device == "cuda" else False,
    logging_steps=5,
    eval_strategy="steps",
    eval_steps=10,
    save_steps=20,
    save_total_limit=2,
    load_best_model_at_end=True,
    report_to="none",  # No usar wandb/tensorboard
)

# =====================================================
# PASO 6: Funci√≥n de formateo de datos
# =====================================================
def format_prompts(examples):
    texts = []
    for messages in examples["messages"]:
        # Convertir formato messages a texto
        text = ""
        for msg in messages:
            if msg["role"] == "system":
                text += f"<|system|>\n{msg['content']}\n"
            elif msg["role"] == "user":
                text += f"<|user|>\n{msg['content']}\n"
            elif msg["role"] == "assistant":
                text += f"<|assistant|>\n{msg['content']}\n"
        texts.append(text)
    
    return tokenizer(texts, truncation=True, padding="max_length", max_length=512)

# Aplicar formateo
train_dataset = train_dataset.map(format_prompts, batched=True, remove_columns=train_dataset.column_names)
val_dataset = val_dataset.map(format_prompts, batched=True, remove_columns=val_dataset.column_names)

# =====================================================
# PASO 7: Crear trainer
# =====================================================
print("\nüèãÔ∏è Creando trainer...")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# =====================================================
# PASO 8: Entrenar!
# =====================================================
print("\nüöÄ Iniciando training...")
print("=" * 60)
print("‚è∞ Esto tomar√° aproximadamente 30-60 minutos seg√∫n tu hardware")
print("=" * 60)

trainer.train()

# =====================================================
# PASO 9: Guardar modelo
# =====================================================
print("\nüíæ Guardando modelo fine-tuned...")

OUTPUT_DIR = "./tinyllama-intent-classifier-final"
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"‚úÖ Modelo guardado en: {OUTPUT_DIR}")

# =====================================================
# PASO 10: Evaluar
# =====================================================
print("\nüìä Evaluando modelo en validation set...")

eval_results = trainer.evaluate()
print(f"\nüìà Resultados de evaluaci√≥n:")
for key, value in eval_results.items():
    print(f"   {key}: {value:.4f}")

print("\n" + "=" * 60)
print("üéâ FINE-TUNING COMPLETADO!")
print("=" * 60)
print(f"‚úÖ Modelo entrenado guardado en: llama-3.2-1b-intent-classifier-final")
print(f"‚úÖ Listo para integrar en orquestador_inteligente.py")
